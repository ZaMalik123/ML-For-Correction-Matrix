{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a31c58-ac32-4e6d-8102-d21630e193fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Enhanced GNN Pipeline for First Column Learning\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (GCNConv, GATConv, GINConv, global_mean_pool, \n",
    "                                global_max_pool, global_add_pool, MessagePassing)\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d, LayerNorm\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_aer.noise import depolarizing_error, NoiseModel\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from qiskit_ibm_runtime import Sampler, Session, Options\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "  \n",
    "def hellinger_fidelity(p, q):\n",
    "    \"\"\"Compute Hellinger fidelity between two probability distributions.\n",
    "\n",
    "    The Hellinger fidelity is defined as:\n",
    "        F_H(p, q) = sum_i sqrt(p_i * q_i)\n",
    "\n",
    "    It ranges from 0 (completely different) to 1 (identical).\n",
    "\n",
    "    Args:\n",
    "        p: First probability distribution\n",
    "        q: Second probability distribution\n",
    "\n",
    "    Returns:\n",
    "        Hellinger fidelity (scalar in [0, 1])\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "\n",
    "    # Ensure non-negative\n",
    "    p = np.maximum(p, 0)\n",
    "    q = np.maximum(q, 0)\n",
    "\n",
    "    # Normalize\n",
    "    if np.sum(p) > 0:\n",
    "        p = p / np.sum(p)\n",
    "    if np.sum(q) > 0:\n",
    "        q = q / np.sum(q)\n",
    "\n",
    "    return np.sum(np.sqrt(p * q))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Original helper functions\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def pauli_noise_model(p1=0.01, p2=0.1):\n",
    "    nm = NoiseModel()\n",
    "    one_q_gates = [\"h\",\"s\",\"t\",\"x\",\"y\",\"z\",\"rz\",\"sx\"]\n",
    "    two_q_gates = [\"cx\",\"cz\"]\n",
    "    one_q_err = depolarizing_error(p1, 1)\n",
    "    two_q_err = depolarizing_error(p2, 2)\n",
    "    nm.add_all_qubit_quantum_error(one_q_err, one_q_gates)\n",
    "    nm.add_all_qubit_quantum_error(two_q_err, two_q_gates)\n",
    "    return nm\n",
    "\n",
    "def get_ideal_distribution(circuit, shots=8192):\n",
    "    \"\"\"Get ideal (noiseless) distribution from circuit.\"\"\"\n",
    "    sim = AerSimulator()\n",
    "    full_circuit = circuit.copy()\n",
    "    full_circuit.measure_all()\n",
    "    \n",
    "    result = sim.run(full_circuit, shots=shots).result()\n",
    "    counts = result.get_counts()\n",
    "    \n",
    "    n = circuit.num_qubits\n",
    "    dist = np.zeros(2**n, dtype=np.float32)\n",
    "    for bitstring, c in counts.items():\n",
    "        idx = int(bitstring, 2)\n",
    "        dist[idx] = c / shots\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def random_STHCZ_circuit(n_qubits: int, depth_layers: int, p_cz=0.5):\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    oneq_gates = [\"h\",\"s\",\"t\"]\n",
    "    for _ in range(depth_layers):\n",
    "        for q in range(n_qubits):\n",
    "            g = np.random.choice(oneq_gates)\n",
    "            getattr(qc, g)(q)\n",
    "        if np.random.rand() < p_cz and n_qubits >= 2:\n",
    "            a, b = np.random.choice(n_qubits, size=2, replace=False)\n",
    "            qc.cz(a, b)\n",
    "        qc.barrier()\n",
    "    return qc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Enhanced Circuit to Graph Conversion\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def circuit_to_graph(circuit: QuantumCircuit, max_depth: int = 20) -> Data:\n",
    "    \"\"\"Convert circuit to graph with rich features.\"\"\"\n",
    "    n_qubits = circuit.num_qubits\n",
    "    \n",
    "    # Extended gate types\n",
    "    gate_types = [\"h\", \"s\", \"t\", \"x\", \"y\", \"z\", \"rz\", \"sx\", \"cx\", \"cz\", \"barrier\", \"other\"]\n",
    "    gate_to_idx = {g: i for i, g in enumerate(gate_types)}\n",
    "    \n",
    "    node_features = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    last_gate_on_qubit = [-1] * n_qubits\n",
    "    gate_depth = [0] * n_qubits\n",
    "    \n",
    "    gate_idx = 0\n",
    "    total_depth = 0\n",
    "    \n",
    "    for inst, qargs, cargs in circuit.data:\n",
    "        if inst.name == \"barrier\":\n",
    "            continue\n",
    "            \n",
    "        gate_name = inst.name if inst.name in gate_to_idx else \"other\"\n",
    "        \n",
    "        # Calculate local depth\n",
    "        involved_qubits = [circuit.qubits.index(q) for q in qargs]\n",
    "        local_depth = max(gate_depth[q] for q in involved_qubits) + 1\n",
    "        total_depth = max(total_depth, local_depth)\n",
    "        \n",
    "        for q in involved_qubits:\n",
    "            gate_depth[q] = local_depth\n",
    "        \n",
    "        # Node features\n",
    "        gate_one_hot = [0] * len(gate_types)\n",
    "        gate_one_hot[gate_to_idx[gate_name]] = 1\n",
    "        \n",
    "        qubit_vec = [0] * n_qubits\n",
    "        for q in qargs:\n",
    "            qi = circuit.qubits.index(q)\n",
    "            qubit_vec[qi] = 1\n",
    "        \n",
    "        # Additional features\n",
    "        arity = inst.num_qubits\n",
    "        is_entangling = 1 if inst.num_qubits > 1 else 0\n",
    "        is_clifford = 1 if gate_name in [\"h\", \"s\", \"cx\", \"cz\"] else 0\n",
    "        is_phase_gate = 1 if gate_name in [\"s\", \"t\", \"rz\"] else 0\n",
    "        normalized_depth = local_depth / max(max_depth, 1)\n",
    "        \n",
    "        # For first column: distance from input matters\n",
    "        distance_from_input = local_depth / max(total_depth, 1) if total_depth > 0 else 0\n",
    "        \n",
    "        node_feat = (gate_one_hot + qubit_vec + \n",
    "                    [arity, is_entangling, is_clifford, is_phase_gate, \n",
    "                     normalized_depth, distance_from_input])\n",
    "        node_features.append(node_feat)\n",
    "        \n",
    "        # Edges\n",
    "        for q in qargs:\n",
    "            qi = circuit.qubits.index(q)\n",
    "            if last_gate_on_qubit[qi] != -1:\n",
    "                edge_index.append([last_gate_on_qubit[qi], gate_idx])\n",
    "                edge_attr.append([1, 0, normalized_depth])  # [sequential, entangling, depth]\n",
    "                \n",
    "                edge_index.append([gate_idx, last_gate_on_qubit[qi]])\n",
    "                edge_attr.append([1, 0, normalized_depth])\n",
    "            last_gate_on_qubit[qi] = gate_idx\n",
    "        \n",
    "        # Entangling edges\n",
    "        if inst.num_qubits == 2:\n",
    "            for i, q in enumerate(involved_qubits):\n",
    "                for j, other_q in enumerate(involved_qubits):\n",
    "                    if i != j and last_gate_on_qubit[other_q] != -1:\n",
    "                        edge_index.append([gate_idx, last_gate_on_qubit[other_q]])\n",
    "                        edge_attr.append([0, 1, normalized_depth])\n",
    "        \n",
    "        gate_idx += 1\n",
    "    \n",
    "    # Global features\n",
    "    global_features = [\n",
    "        n_qubits / 10,\n",
    "        total_depth / 20,\n",
    "        gate_idx / 100,\n",
    "        len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n",
    "    ]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if len(node_features) == 0:\n",
    "        feature_dim = len(gate_types) + n_qubits + 6\n",
    "        x = torch.zeros((1, feature_dim), dtype=torch.float32)\n",
    "        edge_index_tensor = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr_tensor = torch.zeros((0, 3), dtype=torch.float32)\n",
    "    else:\n",
    "        x = torch.tensor(node_features, dtype=torch.float32)\n",
    "        if len(edge_index) > 0:\n",
    "            edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "            edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "        else:\n",
    "            edge_index_tensor = torch.zeros((2, 0), dtype=torch.long)\n",
    "            edge_attr_tensor = torch.zeros((0, 3), dtype=torch.float32)\n",
    "    \n",
    "    global_feat_tensor = torch.tensor(global_features, dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index_tensor, edge_attr=edge_attr_tensor,\n",
    "                global_features=global_feat_tensor)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Dataset builder\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from dec_matrix_computer import DECMatrixComputer\n",
    "\n",
    "def build_graph_dataset(dec_computer, n_qubits=3, depth_layers=6, n_samples=1000,\n",
    "                       shots=8192, transpile_circuit=False, basis_gates=None, twirl=True):\n",
    "    \"\"\"Build dataset with graph representations and first column targets.\"\"\"\n",
    "    graphs = []\n",
    "    columns = []\n",
    "    circuits = []\n",
    "    ideal_dists = []\n",
    "    noisy_dists = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate circuit\n",
    "        circ = random_STHCZ_circuit(n_qubits, depth_layers)\n",
    "\n",
    "        if transpile_circuit:\n",
    "            if basis_gates is None:\n",
    "                basis_gates = ['cx', 'id', 'rz', 'sx', 'x']  # IBM basis\n",
    "            \n",
    "            circ = transpile(\n",
    "                circ, \n",
    "                basis_gates=basis_gates,\n",
    "                optimization_level=0  # No optimization to preserve structure\n",
    "            )\n",
    "        \n",
    "        # Convert to graph\n",
    "        graph = circuit_to_graph(circ, max_depth=depth_layers)\n",
    "\n",
    "        # Get first column (for NEC)\n",
    "        first_col = dec_computer.compute_first_column_fast(circ)\n",
    "        \n",
    "        # Get ideal distribution\n",
    "        ideal_dist = get_ideal_distribution(circ, shots)\n",
    "        \n",
    "        # Store\n",
    "        graph.y = torch.tensor(first_col, dtype=torch.float32).unsqueeze(0)\n",
    "        graphs.append(graph)\n",
    "        columns.append(first_col)\n",
    "        circuits.append(circ)\n",
    "        ideal_dists.append(ideal_dist)\n",
    "    \n",
    "    return graphs, columns, circuits, ideal_dists\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Training function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_column_gnn(graphs, n_qubits, n_train=800, epochs=200, batch_size=32,\n",
    "                    lr=1e-3, model_type=\"GNN\", loss_type=\"mse\",\n",
    "                    weight_decay=1e-4, scheduler_patience=10):\n",
    "    \"\"\"Train GNN for column prediction.\"\"\"\n",
    "    \n",
    "    from graph_networks import ColumnGNN, ColumnGAT, ColumnGIN\n",
    "    from losses import ColumnL2Loss, StochasticColumnLoss, KLDivergenceLoss\n",
    "    \n",
    "    # Split data\n",
    "    train_graphs = graphs[:n_train]\n",
    "    val_graphs = graphs[n_train:]\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model dimensions\n",
    "    node_feat_dim = train_graphs[0].x.shape[1]\n",
    "    output_dim = 2 ** n_qubits\n",
    "    hidden_dim = 128\n",
    "    global_feat_dim = train_graphs[0].global_features.shape[0]\n",
    "    \n",
    "    # Initialize model\n",
    "    if model_type == \"GNN\":\n",
    "        model = ColumnGNN(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    elif model_type == \"GAT\":\n",
    "        model = ColumnGAT(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    elif model_type == \"GIN\":\n",
    "        model = ColumnGIN(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Loss function\n",
    "    if loss_type == \"l2\":\n",
    "        loss_fn = ColumnL2Loss()\n",
    "    elif loss_type == \"stochastic\":\n",
    "        loss_fn = StochasticColumnLoss()\n",
    "    elif loss_type == \"kl\":\n",
    "        loss_fn = KLDivergenceLoss()\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=scheduler_patience, factor=0.5\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_size_actual = batch.num_graphs\n",
    "            global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "            \n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            target = batch.y.view(-1, output_dim)\n",
    "            \n",
    "            loss = loss_fn(out, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "        train_loss /= len(train_graphs)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_size_actual = batch.num_graphs\n",
    "                global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "                \n",
    "                out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "                target = batch.y.view(-1, output_dim)\n",
    "                \n",
    "                loss = loss_fn(out, target)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "        val_loss /= len(val_graphs)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Train: {train_loss:.4e} | Val: {val_loss:.4e} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 30:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Evaluation function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_column_model(model, val_graphs, n_qubits):\n",
    "    \"\"\"Evaluate column prediction model.\"\"\"\n",
    "    model.eval()\n",
    "    val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    output_dim = 2 ** n_qubits\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_size_actual = batch.num_graphs\n",
    "            global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "            \n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            target = batch.y.view(-1, output_dim)\n",
    "            \n",
    "            all_preds.append(out.cpu().numpy())\n",
    "            all_true.append(target.cpu().numpy())\n",
    "    \n",
    "    Y_pred = np.concatenate(all_preds, axis=0)\n",
    "    Y_true = np.concatenate(all_true, axis=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # MSE and MAE\n",
    "    metrics['mse'] = np.mean((Y_pred - Y_true) ** 2)\n",
    "    metrics['mae'] = np.mean(np.abs(Y_pred - Y_true))\n",
    "    \n",
    "    # L2 norm error\n",
    "    l2_errors = np.sqrt(np.sum((Y_pred - Y_true) ** 2, axis=1))\n",
    "    metrics['mean_l2'] = np.mean(l2_errors)\n",
    "    metrics['std_l2'] = np.std(l2_errors)\n",
    "    \n",
    "    # Stochasticity error\n",
    "    sum_pred = np.sum(Y_pred, axis=1)\n",
    "    metrics['stochastic_error'] = np.mean(np.abs(sum_pred - 1.0))\n",
    "    \n",
    "    # KL divergence (treating as distributions)\n",
    "    kl_divs = []\n",
    "    for i in range(len(Y_pred)):\n",
    "        pred = Y_pred[i] + 1e-10\n",
    "        true = Y_true[i] + 1e-10\n",
    "        pred_norm = pred / pred.sum()\n",
    "        true_norm = true / true.sum()\n",
    "        kl = np.sum(true_norm * np.log(true_norm / pred_norm))\n",
    "        kl_divs.append(kl)\n",
    "    metrics['mean_kl'] = np.mean(kl_divs)\n",
    "    metrics['std_kl'] = np.std(kl_divs)\n",
    "    \n",
    "    # Total variation distance\n",
    "    tv_distances = 0.5 * np.sum(np.abs(Y_pred - Y_true), axis=1)\n",
    "    metrics['mean_tv'] = np.mean(tv_distances)\n",
    "    \n",
    "    return metrics, Y_pred, Y_true\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Correction Evaluation Function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_correction(model, val_graphs, val_columns, val_circuits, val_ideal_dists, \n",
    "                       n_qubits, dec_computer,shots=8192):\n",
    "    \"\"\"Evaluate correction performance using both true and predicted columns.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Hellinger fidelities before and after correction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    results = {\n",
    "        'fidelity_uncorrected': [],\n",
    "        'fidelity_true_correction': [],\n",
    "        'fidelity_pred_correction': []\n",
    "    }\n",
    "    \n",
    "    # Get model predictions\n",
    "    val_loader = DataLoader(val_graphs, batch_size=1, shuffle=False)\n",
    "    pred_columns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            global_feats = batch.global_features.view(1, -1)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            pred_columns.append(out.cpu().numpy()[0])\n",
    "    \n",
    "    # Evaluate each circuit\n",
    "    for i, (circ, true_col, pred_col, ideal_dist) in enumerate(zip(\n",
    "            val_circuits, val_columns, pred_columns, val_ideal_dists)):\n",
    "        \n",
    "        # Get noisy distribution \n",
    "        noisy_dist = dec_computer.sample_circuit(circ)\n",
    "        \n",
    "        # Compute uncorrected fidelity\n",
    "        fid_uncorrected = hellinger_fidelity(noisy_dist, ideal_dist)\n",
    "        results['fidelity_uncorrected'].append(fid_uncorrected)\n",
    "        \n",
    "        # Correct using dec_computer and the correct column\n",
    "        corrected_true = dec_computer.correct_distribution_properly(noisy_dist, true_col)\n",
    "        fid_true = hellinger_fidelity(corrected_true, ideal_dist)\n",
    "        results['fidelity_true_correction'].append(fid_true)\n",
    "\n",
    "        # Correct using dec_computer and the predicted column\n",
    "        corrected_pred = dec_computer.correct_distribution_properly(noisy_dist, pred_col)\n",
    "        fid_pred = hellinger_fidelity(corrected_pred, ideal_dist)\n",
    "        results['fidelity_pred_correction'].append(fid_pred)\n",
    "\n",
    "    # Compute statistics\n",
    "    summary = {\n",
    "        'mean_fidelity_uncorrected': np.mean(results['fidelity_uncorrected']),\n",
    "        'std_fidelity_uncorrected': np.std(results['fidelity_uncorrected']),\n",
    "        'mean_fidelity_true_correction': np.mean(results['fidelity_true_correction']),\n",
    "        'std_fidelity_true_correction': np.std(results['fidelity_true_correction']),\n",
    "        'mean_fidelity_pred_correction': np.mean(results['fidelity_pred_correction']),\n",
    "        'std_fidelity_pred_correction': np.std(results['fidelity_pred_correction']),\n",
    "    }\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a977ef5f-4545-47eb-8770-f0ae36825145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (SIMULATED DEPOLARIZING)\n",
      "====================================================================================================\n",
      "\n",
      "Building graph dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_26056\\86377704.py:117: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  for inst, qargs, cargs in circuit.data:\n",
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_26056\\86377704.py:181: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset built: 2000 circuits\n",
      "Node features dim: 23\n",
      "Target dimension: 32\n",
      "\n",
      "======================================================================\n",
      "Training GCN + MSE...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\anaconda3\\envs\\tf\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train: 6.5980e-04 | Val: 4.7824e-04 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 2.7304e-04 | Val: 2.5504e-04 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 1.7315e-04 | Val: 1.7252e-04 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 1.6076e-04 | Val: 3.2410e-04 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.5052e-04 | Val: 3.6090e-04 | LR: 5.00e-04\n",
      "Epoch 050 | Train: 1.4514e-04 | Val: 3.5469e-04 | LR: 2.50e-04\n",
      "Early stopping at epoch 50\n",
      "\n",
      "Evaluating correction for GCN + MSE...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 6.9966e-04 | Val: 4.4874e-04 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 2.7991e-04 | Val: 2.4660e-04 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 1.7764e-04 | Val: 1.8362e-04 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 1.6587e-04 | Val: 2.3842e-04 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.5422e-04 | Val: 2.2946e-04 | LR: 5.00e-04\n",
      "Early stopping at epoch 47\n",
      "\n",
      "Evaluating correction for GCN + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + L2 Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 1.4348e-01 | Val: 1.1876e-01 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 8.9029e-02 | Val: 8.9396e-02 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 7.0910e-02 | Val: 7.3557e-02 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 6.8254e-02 | Val: 7.3322e-02 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 6.6688e-02 | Val: 7.7945e-02 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 6.4932e-02 | Val: 7.3808e-02 | LR: 5.00e-04\n",
      "Epoch 060 | Train: 6.3567e-02 | Val: 7.3994e-02 | LR: 2.50e-04\n",
      "Epoch 070 | Train: 6.3230e-02 | Val: 7.4645e-02 | LR: 1.25e-04\n",
      "Early stopping at epoch 75\n",
      "\n",
      "Evaluating correction for GCN + L2 Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GAT + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 1.5878e-03 | Val: 7.6754e-03 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 2.6783e-04 | Val: 1.8774e-04 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 2.1258e-04 | Val: 1.8977e-04 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 1.9110e-04 | Val: 1.6768e-04 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.6171e-04 | Val: 1.2967e-04 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.3551e-04 | Val: 1.0800e-04 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.1447e-04 | Val: 9.3605e-05 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 1.0875e-04 | Val: 8.3794e-05 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 9.9786e-05 | Val: 8.4612e-05 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 9.5956e-05 | Val: 7.5556e-05 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 8.8599e-05 | Val: 7.3053e-05 | LR: 1.00e-03\n",
      "Epoch 110 | Train: 8.4476e-05 | Val: 6.0514e-05 | LR: 5.00e-04\n",
      "Epoch 120 | Train: 8.1531e-05 | Val: 6.3544e-05 | LR: 5.00e-04\n",
      "Epoch 130 | Train: 8.0751e-05 | Val: 5.8335e-05 | LR: 5.00e-04\n",
      "Epoch 140 | Train: 8.0582e-05 | Val: 5.8560e-05 | LR: 5.00e-04\n",
      "Epoch 150 | Train: 8.1288e-05 | Val: 5.8945e-05 | LR: 5.00e-04\n",
      "Epoch 160 | Train: 7.6339e-05 | Val: 5.6638e-05 | LR: 5.00e-04\n",
      "Epoch 170 | Train: 7.7316e-05 | Val: 5.9766e-05 | LR: 5.00e-04\n",
      "Epoch 180 | Train: 7.5791e-05 | Val: 5.7034e-05 | LR: 5.00e-04\n",
      "Epoch 190 | Train: 7.2960e-05 | Val: 5.0704e-05 | LR: 2.50e-04\n",
      "\n",
      "Evaluating correction for GAT + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GIN + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 1.4668e-03 | Val: 5.0651e-04 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 2.3955e-04 | Val: 2.2852e-04 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 1.7846e-04 | Val: 1.1793e-04 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 1.4522e-04 | Val: 1.1062e-04 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.2355e-04 | Val: 8.1573e-05 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.1516e-04 | Val: 8.8823e-05 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.0509e-04 | Val: 7.6081e-05 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 9.7701e-05 | Val: 7.8650e-05 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 9.6895e-05 | Val: 8.4194e-05 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 9.2429e-05 | Val: 7.2646e-05 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 8.4490e-05 | Val: 5.7929e-05 | LR: 5.00e-04\n",
      "Epoch 110 | Train: 8.3066e-05 | Val: 6.1772e-05 | LR: 5.00e-04\n",
      "Epoch 120 | Train: 8.0944e-05 | Val: 5.5968e-05 | LR: 5.00e-04\n",
      "Epoch 130 | Train: 7.7540e-05 | Val: 5.4999e-05 | LR: 2.50e-04\n",
      "Epoch 140 | Train: 7.6613e-05 | Val: 5.2514e-05 | LR: 2.50e-04\n",
      "Epoch 150 | Train: 7.6135e-05 | Val: 5.3683e-05 | LR: 2.50e-04\n",
      "Epoch 160 | Train: 7.2134e-05 | Val: 5.0567e-05 | LR: 1.25e-04\n",
      "Epoch 170 | Train: 7.2623e-05 | Val: 4.9870e-05 | LR: 6.25e-05\n",
      "Epoch 180 | Train: 7.1828e-05 | Val: 4.9850e-05 | LR: 6.25e-05\n",
      "Epoch 190 | Train: 7.1762e-05 | Val: 4.9483e-05 | LR: 6.25e-05\n",
      "\n",
      "Evaluating correction for GIN + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + KL Divergence...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 2.2561e-01 | Val: 1.8087e-01 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 7.5772e-02 | Val: 7.2225e-02 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 6.9748e-02 | Val: 7.2400e-02 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 6.6709e-02 | Val: 7.2119e-02 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 6.3169e-02 | Val: 6.9629e-02 | LR: 5.00e-04\n",
      "Epoch 050 | Train: 6.1397e-02 | Val: 6.9064e-02 | LR: 2.50e-04\n",
      "Epoch 060 | Train: 6.0390e-02 | Val: 6.7790e-02 | LR: 1.25e-04\n",
      "Early stopping at epoch 67\n",
      "\n",
      "Evaluating correction for GCN + KL Divergence...\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Model + Loss                       MSE          MAE     L2 Error       KL Div    Stoch Err\n",
      "-------------------------------------------------------------------------------------\n",
      "GCN + MSE                   1.7252e-04   8.0835e-03   7.1147e-02   7.2250e-02   5.6475e-08\n",
      "GCN + Stochastic Loss       1.8028e-04   8.3357e-03   7.2873e-02   7.6284e-02   6.5863e-08\n",
      "GCN + L2 Loss               1.5925e-04   7.7606e-03   6.7841e-02   6.8381e-02   6.6459e-08\n",
      "GAT + Stochastic Loss       4.9732e-05   4.1793e-03   3.6788e-02   2.5162e-02   6.6608e-08\n",
      "GIN + Stochastic Loss       4.8820e-05   4.1263e-03   3.6244e-02   2.5305e-02   6.7353e-08\n",
      "GCN + KL Divergence         1.6475e-04   7.9647e-03   6.9763e-02   6.4166e-02   5.9903e-08\n",
      "\n",
      "======================================================================\n",
      "CORRECTION PERFORMANCE (Hellinger Fidelity)\n",
      "======================================================================\n",
      "\n",
      "Model + Loss                  Uncorrected             STD     True Column             STD     Pred Column             STD\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "GCN + MSE                          0.8209          0.1086          0.8823          0.0864          0.9228          0.0422\n",
      "GCN + Stochastic Loss              0.8205          0.1089          0.8812          0.0889          0.9243          0.0440\n",
      "GCN + L2 Loss                      0.8206          0.1090          0.8833          0.0880          0.9227          0.0434\n",
      "GAT + Stochastic Loss              0.8208          0.1085          0.8810          0.0913          0.9054          0.0636\n",
      "GIN + Stochastic Loss              0.8207          0.1087          0.8815          0.0890          0.9019          0.0684\n",
      "GCN + KL Divergence                0.8208          0.1091          0.8836          0.0794          0.9258          0.0430\n",
      "\n",
      "ğŸ† Best Model (by L2 error): GIN + Stochastic Loss\n",
      "   Mean L2 Error: 3.6244e-02\n",
      "   Mean KL Divergence: 2.5305e-02\n",
      "   Stochasticity Error: 6.7353e-08\n",
      "\n",
      "   Correction Performance:\n",
      "   Uncorrected Fidelity: 0.8207 Â± 0.1087\n",
      "   True Column Fidelity: 0.8815 Â± 0.0890\n",
      "   Pred Column Fidelity: 0.9019 Â± 0.0684\n",
      "\n",
      "   Improvement over uncorrected:\n",
      "   True column: 33.9% towards ideal\n",
      "   Pred column: 45.3% towards ideal\n",
      "   Pred achieves 102.3% of true column performance\n",
      "\n",
      "======================================================================\n",
      "DETAILED METRICS\n",
      "======================================================================\n",
      "\n",
      "L2 Error Statistics:\n",
      "  GCN + MSE                : Î¼=7.1147e-02, Ïƒ=2.1421e-02\n",
      "  GCN + Stochastic Loss    : Î¼=7.2873e-02, Ïƒ=2.1413e-02\n",
      "  GCN + L2 Loss            : Î¼=6.7841e-02, Ïƒ=2.2213e-02\n",
      "  GAT + Stochastic Loss    : Î¼=3.6788e-02, Ïƒ=1.5430e-02\n",
      "  GIN + Stochastic Loss    : Î¼=3.6244e-02, Ïƒ=1.5768e-02\n",
      "  GCN + KL Divergence      : Î¼=6.9763e-02, Ïƒ=2.0126e-02\n",
      "\n",
      "KL Divergence Statistics:\n",
      "  GCN + MSE                : Î¼=7.2250e-02, Ïƒ=4.3958e-02\n",
      "  GCN + Stochastic Loss    : Î¼=7.6284e-02, Ïƒ=4.5944e-02\n",
      "  GCN + L2 Loss            : Î¼=6.8381e-02, Ïƒ=4.2448e-02\n",
      "  GAT + Stochastic Loss    : Î¼=2.5162e-02, Ïƒ=1.9627e-02\n",
      "  GIN + Stochastic Loss    : Î¼=2.5305e-02, Ïƒ=1.9920e-02\n",
      "  GCN + KL Divergence      : Î¼=6.4166e-02, Ïƒ=3.7289e-02\n",
      "\n",
      "Total Variation Distance:\n",
      "  GCN + MSE                : 1.2934e-01\n",
      "  GCN + Stochastic Loss    : 1.3337e-01\n",
      "  GCN + L2 Loss            : 1.2417e-01\n",
      "  GAT + Stochastic Loss    : 6.6869e-02\n",
      "  GIN + Stochastic Loss    : 6.6020e-02\n",
      "  GCN + KL Divergence      : 1.2744e-01\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "The GNN approach demonstrates effective noise correction using FWHT:\n",
      "\n",
      "1. Column Prediction Performance:\n",
      "   - Best model: GIN + Stochastic Loss\n",
      "   - Mean L2 error: 3.6244e-02\n",
      "   - Stochasticity maintained: 6.7353e-08\n",
      "\n",
      "2. Correction Performance:\n",
      "   - Uncorrected fidelity: 0.8207\n",
      "   - True column correction: 0.8815 (33.9% improvement)\n",
      "   - Predicted column correction: 0.9019 (45.3% improvement)\n",
      "   - Model achieves 102.3% of oracle performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main comparison script\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings\n",
    "    n_qubits = 5\n",
    "    depth = 5\n",
    "    n_samples = 2000\n",
    "    p1, p2 = 0.05, 0.02\n",
    "    shots = 2048\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (SIMULATED DEPOLARIZING)\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Build dataset\n",
    "    print(\"\\nBuilding graph dataset...\")\n",
    "    nm = pauli_noise_model(p1, p2)\n",
    "    backend = AerSimulator(noise_model=nm)\n",
    "    dec_computer = DECMatrixComputer(backend=backend, shots=shots, twirl=False)\n",
    "    \n",
    "    graphs, columns, circuits, ideal_dists = build_graph_dataset(\n",
    "         dec_computer, n_qubits, depth, n_samples, shots=shots, transpile_circuit=True\n",
    "    )\n",
    "    print(f\"Dataset built: {len(graphs)} circuits\")\n",
    "    print(f\"Node features dim: {graphs[0].x.shape[1]}\")\n",
    "    print(f\"Target dimension: {2**n_qubits}\")\n",
    "    \n",
    "    n_train = int(0.8 * n_samples)\n",
    "    val_graphs = graphs[n_train:]\n",
    "    val_columns = columns[n_train:]\n",
    "    val_circuits = circuits[n_train:]\n",
    "    val_ideal_dists = ideal_dists[n_train:]\n",
    "\n",
    "    results = {}\n",
    "    correction_results = {}\n",
    "    \n",
    "    # Test different combinations\n",
    "    experiments = [\n",
    "        (\"GNN\", \"mse\", \"GCN + MSE\"),\n",
    "        (\"GNN\", \"stochastic\", \"GCN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"l2\", \"GCN + L2 Loss\"),\n",
    "        (\"GAT\", \"stochastic\", \"GAT + Stochastic Loss\"),\n",
    "        (\"GIN\", \"stochastic\", \"GIN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"kl\", \"GCN + KL Divergence\")\n",
    "    ]\n",
    "    \n",
    "    for model_type, loss_type, name in experiments:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Training {name}...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        model, train_losses, val_losses = train_column_gnn(\n",
    "            graphs, n_qubits, n_train=n_train, epochs=200,\n",
    "            batch_size=32, lr=1e-3, model_type=model_type, loss_type=loss_type\n",
    "        )\n",
    "        \n",
    "        metrics, preds, true = evaluate_column_model(model, val_graphs, n_qubits)\n",
    "        results[name] = metrics\n",
    "        \n",
    "        # Evaluate correction performance\n",
    "        print(f\"\\nEvaluating correction for {name}...\")\n",
    "        corr_res, corr_summary = evaluate_correction(\n",
    "            model, val_graphs, val_columns, val_circuits, val_ideal_dists,\n",
    "            n_qubits, dec_computer, shots\n",
    "        )\n",
    "        correction_results[name] = corr_summary\n",
    "\n",
    "    # Print comprehensive results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n{:<25} {:>12} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "        \"Model + Loss\", \"MSE\", \"MAE\", \"L2 Error\", \"KL Div\", \"Stoch Err\"\n",
    "    ))\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        print(\"{:<25} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e}\".format(\n",
    "            name,\n",
    "            metrics['mse'],\n",
    "            metrics['mae'],\n",
    "            metrics['mean_l2'],\n",
    "            metrics['mean_kl'],\n",
    "            metrics['stochastic_error']\n",
    "        ))\n",
    "    \n",
    "    # Print correction results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CORRECTION PERFORMANCE (Hellinger Fidelity)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n{:<25} {:>15} {:>15} {:>15} {:>15} {:>15} {:>15}\".format(\n",
    "        \"Model + Loss\", \"Uncorrected\", \"STD\", \"True Column\", \"STD\", \"Pred Column\", \"STD\"\n",
    "    ))\n",
    "    print(\"-\" * 144)\n",
    "    \n",
    "    for name, corr in correction_results.items():\n",
    "        print(\"{:<25} {:>15.4f} {:>15.4f} {:>15.4f} {:>15.4f} {:>15.4f} {:>15.4f}\".format(\n",
    "            name,\n",
    "            corr['mean_fidelity_uncorrected'],\n",
    "            corr['std_fidelity_uncorrected'],\n",
    "            corr['mean_fidelity_true_correction'],\n",
    "            corr['std_fidelity_true_correction'],\n",
    "            corr['mean_fidelity_pred_correction'],\n",
    "            corr['std_fidelity_pred_correction']\n",
    "        ))\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mean_l2'])\n",
    "    print(f\"\\nğŸ† Best Model (by L2 error): {best_model_name}\")\n",
    "    print(f\"   Mean L2 Error: {results[best_model_name]['mean_l2']:.4e}\")\n",
    "    print(f\"   Mean KL Divergence: {results[best_model_name]['mean_kl']:.4e}\")\n",
    "    print(f\"   Stochasticity Error: {results[best_model_name]['stochastic_error']:.4e}\")\n",
    "    \n",
    "    # Best correction performance\n",
    "    best_corr = correction_results[best_model_name]\n",
    "    print(f\"\\n   Correction Performance:\")\n",
    "    print(f\"   Uncorrected Fidelity: {best_corr['mean_fidelity_uncorrected']:.4f} Â± {best_corr['std_fidelity_uncorrected']:.4f}\")\n",
    "    print(f\"   True Column Fidelity: {best_corr['mean_fidelity_true_correction']:.4f} Â± {best_corr['std_fidelity_true_correction']:.4f}\")\n",
    "    print(f\"   Pred Column Fidelity: {best_corr['mean_fidelity_pred_correction']:.4f} Â± {best_corr['std_fidelity_pred_correction']:.4f}\")\n",
    "    \n",
    "    # Improvement metrics\n",
    "    uncorr_fid = best_corr['mean_fidelity_uncorrected']\n",
    "    true_fid = best_corr['mean_fidelity_true_correction']\n",
    "    pred_fid = best_corr['mean_fidelity_pred_correction']\n",
    "    \n",
    "    true_improvement = (true_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_improvement = (pred_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_vs_true = pred_fid / true_fid * 100\n",
    "    \n",
    "    print(f\"\\n   Improvement over uncorrected:\")\n",
    "    print(f\"   True column: {true_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred column: {pred_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred achieves {pred_vs_true:.1f}% of true column performance\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nL2 Error Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_l2']:.4e}, Ïƒ={metrics['std_l2']:.4e}\")\n",
    "    \n",
    "    print(\"\\nKL Divergence Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_kl']:.4e}, Ïƒ={metrics['std_kl']:.4e}\")\n",
    "    \n",
    "    print(\"\\nTotal Variation Distance:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: {metrics['mean_tv']:.4e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\"\"\n",
    "The GNN approach demonstrates effective noise correction using FWHT:\n",
    "\n",
    "1. Column Prediction Performance:\n",
    "   - Best model: {best_model_name}\n",
    "   - Mean L2 error: {results[best_model_name]['mean_l2']:.4e}\n",
    "   - Stochasticity maintained: {results[best_model_name]['stochastic_error']:.4e}\n",
    "\n",
    "2. Correction Performance:\n",
    "   - Uncorrected fidelity: {uncorr_fid:.4f}\n",
    "   - True column correction: {true_fid:.4f} ({true_improvement:.1f}% improvement)\n",
    "   - Predicted column correction: {pred_fid:.4f} ({pred_improvement:.1f}% improvement)\n",
    "   - Model achieves {pred_vs_true:.1f}% of oracle performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1aa3a1-65c0-449c-93a6-c656bbaaaeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (FAKE ATHENS)\n",
      "====================================================================================================\n",
      "\n",
      "Building graph dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_8208\\338620528.py:117: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  for inst, qargs, cargs in circuit.data:\n",
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_8208\\338620528.py:181: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n",
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_8208\\338620528.py:117: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  for inst, qargs, cargs in circuit.data:\n",
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_8208\\338620528.py:181: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset built: 2000 circuits\n",
      "Node features dim: 21\n",
      "Target dimension: 8\n",
      "\n",
      "======================================================================\n",
      "Training GCN + MSE...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\anaconda3\\envs\\tf\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train: 9.3177e-03 | Val: 8.8769e-03 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 5.7180e-03 | Val: 6.2648e-03 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 4.5453e-03 | Val: 4.2960e-03 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 2.4624e-03 | Val: 2.3199e-03 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.9601e-03 | Val: 2.0187e-03 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.9216e-03 | Val: 1.9788e-03 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.8677e-03 | Val: 1.9509e-03 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 1.8452e-03 | Val: 1.9336e-03 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 1.7969e-03 | Val: 1.8565e-03 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 1.7225e-03 | Val: 1.7831e-03 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 1.6644e-03 | Val: 1.6779e-03 | LR: 1.00e-03\n",
      "Epoch 110 | Train: 1.5766e-03 | Val: 1.6552e-03 | LR: 1.00e-03\n",
      "Epoch 120 | Train: 1.5227e-03 | Val: 1.6195e-03 | LR: 1.00e-03\n",
      "Epoch 130 | Train: 1.4516e-03 | Val: 1.5631e-03 | LR: 5.00e-04\n",
      "Epoch 140 | Train: 1.4307e-03 | Val: 1.5272e-03 | LR: 5.00e-04\n",
      "Epoch 150 | Train: 1.3681e-03 | Val: 1.4991e-03 | LR: 5.00e-04\n",
      "Epoch 160 | Train: 1.3281e-03 | Val: 1.4784e-03 | LR: 5.00e-04\n",
      "Epoch 170 | Train: 1.2909e-03 | Val: 1.4632e-03 | LR: 5.00e-04\n",
      "Epoch 180 | Train: 1.2924e-03 | Val: 1.4956e-03 | LR: 5.00e-04\n",
      "Epoch 190 | Train: 1.2604e-03 | Val: 1.4646e-03 | LR: 5.00e-04\n",
      "Epoch 200 | Train: 1.2114e-03 | Val: 1.3728e-03 | LR: 2.50e-04\n",
      "Epoch 210 | Train: 1.1938e-03 | Val: 1.3846e-03 | LR: 2.50e-04\n",
      "Epoch 220 | Train: 1.1722e-03 | Val: 1.3654e-03 | LR: 2.50e-04\n",
      "Epoch 230 | Train: 1.1393e-03 | Val: 1.3764e-03 | LR: 2.50e-04\n",
      "Epoch 240 | Train: 1.1393e-03 | Val: 1.3380e-03 | LR: 2.50e-04\n",
      "Epoch 250 | Train: 1.1284e-03 | Val: 1.3749e-03 | LR: 2.50e-04\n",
      "Epoch 260 | Train: 1.1137e-03 | Val: 1.3160e-03 | LR: 1.25e-04\n",
      "Epoch 270 | Train: 1.0939e-03 | Val: 1.3152e-03 | LR: 1.25e-04\n",
      "Epoch 280 | Train: 1.0633e-03 | Val: 1.2946e-03 | LR: 6.25e-05\n",
      "Epoch 290 | Train: 1.0774e-03 | Val: 1.2937e-03 | LR: 6.25e-05\n",
      "Epoch 300 | Train: 1.0621e-03 | Val: 1.2896e-03 | LR: 6.25e-05\n",
      "Epoch 310 | Train: 1.0637e-03 | Val: 1.2900e-03 | LR: 6.25e-05\n",
      "Epoch 320 | Train: 1.0499e-03 | Val: 1.2817e-03 | LR: 6.25e-05\n",
      "Epoch 330 | Train: 1.0438e-03 | Val: 1.2801e-03 | LR: 3.13e-05\n",
      "Epoch 340 | Train: 1.0485e-03 | Val: 1.2798e-03 | LR: 3.13e-05\n",
      "Epoch 350 | Train: 1.0622e-03 | Val: 1.2753e-03 | LR: 1.56e-05\n",
      "Epoch 360 | Train: 1.0447e-03 | Val: 1.2817e-03 | LR: 7.81e-06\n",
      "Early stopping at epoch 368\n",
      "\n",
      "Evaluating correction for GCN + MSE...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 9.2280e-03 | Val: 8.5727e-03 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 5.7364e-03 | Val: 5.9042e-03 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 4.5188e-03 | Val: 4.2739e-03 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 2.9400e-03 | Val: 2.6469e-03 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 2.0727e-03 | Val: 2.0373e-03 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.9442e-03 | Val: 1.9612e-03 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.9040e-03 | Val: 1.9397e-03 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 1.8465e-03 | Val: 1.9414e-03 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 1.8704e-03 | Val: 1.9190e-03 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 1.7597e-03 | Val: 1.8554e-03 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 1.6879e-03 | Val: 1.7649e-03 | LR: 1.00e-03\n",
      "Epoch 110 | Train: 1.7297e-03 | Val: 1.7762e-03 | LR: 1.00e-03\n",
      "Epoch 120 | Train: 1.6041e-03 | Val: 1.6978e-03 | LR: 1.00e-03\n",
      "Epoch 130 | Train: 1.6050e-03 | Val: 1.6626e-03 | LR: 1.00e-03\n",
      "Epoch 140 | Train: 1.5146e-03 | Val: 1.6493e-03 | LR: 1.00e-03\n",
      "Epoch 150 | Train: 1.4380e-03 | Val: 1.5679e-03 | LR: 1.00e-03\n",
      "Epoch 160 | Train: 1.4042e-03 | Val: 1.5624e-03 | LR: 1.00e-03\n",
      "Epoch 170 | Train: 1.3431e-03 | Val: 1.5175e-03 | LR: 1.00e-03\n",
      "Epoch 180 | Train: 1.2978e-03 | Val: 1.5424e-03 | LR: 1.00e-03\n",
      "Epoch 190 | Train: 1.2926e-03 | Val: 1.4674e-03 | LR: 1.00e-03\n",
      "Epoch 200 | Train: 1.1775e-03 | Val: 1.3922e-03 | LR: 5.00e-04\n",
      "Epoch 210 | Train: 1.1541e-03 | Val: 1.3939e-03 | LR: 5.00e-04\n",
      "Epoch 220 | Train: 1.1322e-03 | Val: 1.3628e-03 | LR: 5.00e-04\n",
      "Epoch 230 | Train: 1.0956e-03 | Val: 1.3788e-03 | LR: 5.00e-04\n",
      "Epoch 240 | Train: 1.0637e-03 | Val: 1.3716e-03 | LR: 2.50e-04\n",
      "Epoch 250 | Train: 1.0489e-03 | Val: 1.3197e-03 | LR: 2.50e-04\n",
      "Epoch 260 | Train: 1.0290e-03 | Val: 1.3237e-03 | LR: 2.50e-04\n",
      "Epoch 270 | Train: 1.0279e-03 | Val: 1.2826e-03 | LR: 2.50e-04\n",
      "Epoch 280 | Train: 1.0049e-03 | Val: 1.2916e-03 | LR: 1.25e-04\n",
      "Epoch 290 | Train: 1.0009e-03 | Val: 1.2609e-03 | LR: 1.25e-04\n",
      "Epoch 300 | Train: 9.8576e-04 | Val: 1.2667e-03 | LR: 6.25e-05\n",
      "Epoch 310 | Train: 9.8345e-04 | Val: 1.2667e-03 | LR: 3.13e-05\n",
      "Early stopping at epoch 315\n",
      "\n",
      "Evaluating correction for GCN + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + L2 Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 2.3004e-01 | Val: 2.3191e-01 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 1.9053e-01 | Val: 1.9422e-01 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 1.8022e-01 | Val: 1.7776e-01 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 1.4338e-01 | Val: 1.4535e-01 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.2484e-01 | Val: 1.2852e-01 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.1023e-01 | Val: 1.1182e-01 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.0691e-01 | Val: 1.0698e-01 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 1.0467e-01 | Val: 1.0539e-01 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 1.0259e-01 | Val: 1.0281e-01 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 9.8657e-02 | Val: 9.8737e-02 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 9.5699e-02 | Val: 9.4907e-02 | LR: 1.00e-03\n",
      "Epoch 110 | Train: 9.3958e-02 | Val: 9.3518e-02 | LR: 1.00e-03\n",
      "Epoch 120 | Train: 9.2480e-02 | Val: 9.2077e-02 | LR: 1.00e-03\n",
      "Epoch 130 | Train: 9.1747e-02 | Val: 9.4528e-02 | LR: 1.00e-03\n",
      "Epoch 140 | Train: 9.1445e-02 | Val: 9.3234e-02 | LR: 1.00e-03\n",
      "Epoch 150 | Train: 8.9941e-02 | Val: 9.2338e-02 | LR: 1.00e-03\n",
      "Epoch 160 | Train: 9.0372e-02 | Val: 9.0574e-02 | LR: 1.00e-03\n",
      "Epoch 170 | Train: 8.8026e-02 | Val: 8.9301e-02 | LR: 5.00e-04\n",
      "Epoch 180 | Train: 8.6898e-02 | Val: 8.8764e-02 | LR: 2.50e-04\n",
      "Epoch 190 | Train: 8.6384e-02 | Val: 8.8291e-02 | LR: 2.50e-04\n",
      "Epoch 200 | Train: 8.5918e-02 | Val: 8.8707e-02 | LR: 2.50e-04\n",
      "Epoch 210 | Train: 8.5224e-02 | Val: 8.8354e-02 | LR: 1.25e-04\n",
      "Epoch 220 | Train: 8.5500e-02 | Val: 8.8146e-02 | LR: 1.25e-04\n",
      "Epoch 230 | Train: 8.4889e-02 | Val: 8.8213e-02 | LR: 1.25e-04\n",
      "Epoch 240 | Train: 8.4717e-02 | Val: 8.7805e-02 | LR: 1.25e-04\n",
      "Epoch 250 | Train: 8.4885e-02 | Val: 8.7930e-02 | LR: 1.25e-04\n",
      "Epoch 260 | Train: 8.4134e-02 | Val: 8.7374e-02 | LR: 1.25e-04\n",
      "Epoch 270 | Train: 8.3591e-02 | Val: 8.7289e-02 | LR: 1.25e-04\n",
      "Epoch 280 | Train: 8.3216e-02 | Val: 8.7023e-02 | LR: 1.25e-04\n",
      "Epoch 290 | Train: 8.3171e-02 | Val: 8.6464e-02 | LR: 1.25e-04\n",
      "Epoch 300 | Train: 8.2618e-02 | Val: 8.6451e-02 | LR: 1.25e-04\n",
      "Epoch 310 | Train: 8.2239e-02 | Val: 8.5782e-02 | LR: 1.25e-04\n",
      "Epoch 320 | Train: 8.1771e-02 | Val: 8.6021e-02 | LR: 1.25e-04\n",
      "Epoch 330 | Train: 8.1446e-02 | Val: 8.5709e-02 | LR: 6.25e-05\n",
      "Epoch 340 | Train: 8.1382e-02 | Val: 8.5517e-02 | LR: 3.13e-05\n",
      "Epoch 350 | Train: 8.1131e-02 | Val: 8.5584e-02 | LR: 3.13e-05\n",
      "Epoch 360 | Train: 8.0903e-02 | Val: 8.5540e-02 | LR: 1.56e-05\n",
      "Epoch 370 | Train: 8.0873e-02 | Val: 8.5442e-02 | LR: 7.81e-06\n",
      "Epoch 380 | Train: 8.0450e-02 | Val: 8.5435e-02 | LR: 7.81e-06\n",
      "Epoch 390 | Train: 8.0509e-02 | Val: 8.5391e-02 | LR: 3.91e-06\n",
      "Epoch 400 | Train: 8.1058e-02 | Val: 8.5366e-02 | LR: 1.95e-06\n",
      "Early stopping at epoch 403\n",
      "\n",
      "Evaluating correction for GCN + L2 Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GAT + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 8.1216e-03 | Val: 6.6178e-03 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 3.2478e-03 | Val: 3.6844e-03 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 2.5897e-03 | Val: 2.1734e-03 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 2.6577e-03 | Val: 2.1937e-03 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 2.2231e-03 | Val: 2.2003e-03 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 2.0273e-03 | Val: 1.8229e-03 | LR: 5.00e-04\n",
      "Epoch 060 | Train: 1.8655e-03 | Val: 1.7481e-03 | LR: 5.00e-04\n",
      "Epoch 070 | Train: 1.9018e-03 | Val: 1.7428e-03 | LR: 5.00e-04\n",
      "Epoch 080 | Train: 1.9542e-03 | Val: 1.7189e-03 | LR: 5.00e-04\n",
      "Epoch 090 | Train: 1.8402e-03 | Val: 1.6904e-03 | LR: 5.00e-04\n",
      "Epoch 100 | Train: 1.7662e-03 | Val: 1.6799e-03 | LR: 5.00e-04\n",
      "Epoch 110 | Train: 1.8500e-03 | Val: 1.6819e-03 | LR: 5.00e-04\n",
      "Epoch 120 | Train: 1.5784e-03 | Val: 1.6511e-03 | LR: 5.00e-04\n",
      "Epoch 130 | Train: 1.7802e-03 | Val: 1.6514e-03 | LR: 2.50e-04\n",
      "Epoch 140 | Train: 1.6942e-03 | Val: 1.6091e-03 | LR: 2.50e-04\n",
      "Epoch 150 | Train: 1.6657e-03 | Val: 1.6201e-03 | LR: 2.50e-04\n",
      "Epoch 160 | Train: 1.5762e-03 | Val: 1.5876e-03 | LR: 2.50e-04\n",
      "Epoch 170 | Train: 1.4829e-03 | Val: 1.5755e-03 | LR: 1.25e-04\n",
      "Epoch 180 | Train: 1.5213e-03 | Val: 1.5686e-03 | LR: 1.25e-04\n",
      "Epoch 190 | Train: 1.6116e-03 | Val: 1.5921e-03 | LR: 1.25e-04\n",
      "Epoch 200 | Train: 1.5401e-03 | Val: 1.5576e-03 | LR: 1.25e-04\n",
      "Epoch 210 | Train: 1.4912e-03 | Val: 1.5692e-03 | LR: 6.25e-05\n",
      "Epoch 220 | Train: 1.5193e-03 | Val: 1.5631e-03 | LR: 3.13e-05\n",
      "Epoch 230 | Train: 1.4336e-03 | Val: 1.5530e-03 | LR: 3.13e-05\n",
      "Epoch 240 | Train: 1.4718e-03 | Val: 1.5511e-03 | LR: 3.13e-05\n",
      "Epoch 250 | Train: 1.4821e-03 | Val: 1.5685e-03 | LR: 1.56e-05\n",
      "Early stopping at epoch 258\n",
      "\n",
      "Evaluating correction for GAT + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GIN + Stochastic Loss...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 7.5683e-03 | Val: 5.3834e-03 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 3.0956e-03 | Val: 2.4262e-03 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 2.3581e-03 | Val: 2.1101e-03 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 2.1762e-03 | Val: 2.0509e-03 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 1.9757e-03 | Val: 1.8212e-03 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 1.8336e-03 | Val: 1.8177e-03 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 1.8731e-03 | Val: 1.7514e-03 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 1.8109e-03 | Val: 1.7898e-03 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 1.6075e-03 | Val: 1.6675e-03 | LR: 5.00e-04\n",
      "Epoch 090 | Train: 1.5884e-03 | Val: 1.6818e-03 | LR: 5.00e-04\n",
      "Epoch 100 | Train: 1.5767e-03 | Val: 1.6298e-03 | LR: 5.00e-04\n",
      "Epoch 110 | Train: 1.5646e-03 | Val: 1.6416e-03 | LR: 5.00e-04\n",
      "Epoch 120 | Train: 1.4988e-03 | Val: 1.6162e-03 | LR: 2.50e-04\n",
      "Epoch 130 | Train: 1.5073e-03 | Val: 1.6217e-03 | LR: 2.50e-04\n",
      "Epoch 140 | Train: 1.4746e-03 | Val: 1.5959e-03 | LR: 2.50e-04\n",
      "Epoch 150 | Train: 1.4593e-03 | Val: 1.6586e-03 | LR: 2.50e-04\n",
      "Epoch 160 | Train: 1.3635e-03 | Val: 1.5710e-03 | LR: 1.25e-04\n",
      "Epoch 170 | Train: 1.3986e-03 | Val: 1.5778e-03 | LR: 1.25e-04\n",
      "Epoch 180 | Train: 1.3715e-03 | Val: 1.5723e-03 | LR: 1.25e-04\n",
      "Epoch 190 | Train: 1.3375e-03 | Val: 1.5576e-03 | LR: 6.25e-05\n",
      "Epoch 200 | Train: 1.3404e-03 | Val: 1.5485e-03 | LR: 3.13e-05\n",
      "Epoch 210 | Train: 1.2865e-03 | Val: 1.5608e-03 | LR: 1.56e-05\n",
      "Epoch 220 | Train: 1.3045e-03 | Val: 1.5556e-03 | LR: 7.81e-06\n",
      "Early stopping at epoch 228\n",
      "\n",
      "Evaluating correction for GIN + Stochastic Loss...\n",
      "\n",
      "======================================================================\n",
      "Training GCN + KL Divergence...\n",
      "======================================================================\n",
      "Epoch 000 | Train: 2.9146e-01 | Val: 2.7283e-01 | LR: 1.00e-03\n",
      "Epoch 010 | Train: 9.2440e-02 | Val: 8.2283e-02 | LR: 1.00e-03\n",
      "Epoch 020 | Train: 6.0390e-02 | Val: 6.0073e-02 | LR: 1.00e-03\n",
      "Epoch 030 | Train: 6.0099e-02 | Val: 5.9642e-02 | LR: 1.00e-03\n",
      "Epoch 040 | Train: 5.7807e-02 | Val: 5.9647e-02 | LR: 1.00e-03\n",
      "Epoch 050 | Train: 5.8649e-02 | Val: 5.8604e-02 | LR: 1.00e-03\n",
      "Epoch 060 | Train: 5.5971e-02 | Val: 5.6577e-02 | LR: 1.00e-03\n",
      "Epoch 070 | Train: 5.3701e-02 | Val: 5.4585e-02 | LR: 1.00e-03\n",
      "Epoch 080 | Train: 5.2672e-02 | Val: 5.5446e-02 | LR: 1.00e-03\n",
      "Epoch 090 | Train: 4.9545e-02 | Val: 5.2330e-02 | LR: 1.00e-03\n",
      "Epoch 100 | Train: 5.0448e-02 | Val: 5.1729e-02 | LR: 1.00e-03\n",
      "Epoch 110 | Train: 4.5252e-02 | Val: 4.9869e-02 | LR: 5.00e-04\n",
      "Epoch 120 | Train: 4.4170e-02 | Val: 4.9251e-02 | LR: 5.00e-04\n",
      "Epoch 130 | Train: 4.3951e-02 | Val: 4.9378e-02 | LR: 5.00e-04\n",
      "Epoch 140 | Train: 4.2124e-02 | Val: 4.7949e-02 | LR: 2.50e-04\n",
      "Epoch 150 | Train: 4.1452e-02 | Val: 4.7860e-02 | LR: 2.50e-04\n",
      "Epoch 160 | Train: 4.0706e-02 | Val: 4.7847e-02 | LR: 2.50e-04\n",
      "Epoch 170 | Train: 4.0137e-02 | Val: 4.7557e-02 | LR: 2.50e-04\n",
      "Epoch 180 | Train: 4.0091e-02 | Val: 4.6872e-02 | LR: 2.50e-04\n",
      "Epoch 190 | Train: 3.9228e-02 | Val: 4.6000e-02 | LR: 2.50e-04\n",
      "Epoch 200 | Train: 3.8670e-02 | Val: 4.5372e-02 | LR: 2.50e-04\n",
      "Epoch 210 | Train: 3.8410e-02 | Val: 4.5244e-02 | LR: 2.50e-04\n",
      "Epoch 220 | Train: 3.8346e-02 | Val: 4.4334e-02 | LR: 2.50e-04\n",
      "Epoch 230 | Train: 3.8121e-02 | Val: 4.5403e-02 | LR: 2.50e-04\n",
      "Epoch 240 | Train: 3.6373e-02 | Val: 4.4170e-02 | LR: 1.25e-04\n",
      "Epoch 250 | Train: 3.6478e-02 | Val: 4.3959e-02 | LR: 1.25e-04\n",
      "Epoch 260 | Train: 3.6231e-02 | Val: 4.4294e-02 | LR: 1.25e-04\n",
      "Epoch 270 | Train: 3.5756e-02 | Val: 4.3592e-02 | LR: 1.25e-04\n",
      "Epoch 280 | Train: 3.5595e-02 | Val: 4.3112e-02 | LR: 1.25e-04\n",
      "Epoch 290 | Train: 3.5534e-02 | Val: 4.2857e-02 | LR: 6.25e-05\n",
      "Epoch 300 | Train: 3.5096e-02 | Val: 4.2750e-02 | LR: 6.25e-05\n",
      "Epoch 310 | Train: 3.5285e-02 | Val: 4.2674e-02 | LR: 6.25e-05\n",
      "Epoch 320 | Train: 3.4773e-02 | Val: 4.2503e-02 | LR: 3.13e-05\n",
      "Epoch 330 | Train: 3.4496e-02 | Val: 4.2419e-02 | LR: 3.13e-05\n",
      "Epoch 340 | Train: 3.5039e-02 | Val: 4.2417e-02 | LR: 1.56e-05\n",
      "Epoch 350 | Train: 3.4384e-02 | Val: 4.2212e-02 | LR: 1.56e-05\n",
      "Epoch 360 | Train: 3.4444e-02 | Val: 4.2246e-02 | LR: 1.56e-05\n",
      "Epoch 370 | Train: 3.4303e-02 | Val: 4.2239e-02 | LR: 7.81e-06\n",
      "Epoch 380 | Train: 3.4461e-02 | Val: 4.2281e-02 | LR: 7.81e-06\n",
      "Epoch 390 | Train: 3.4404e-02 | Val: 4.2257e-02 | LR: 3.91e-06\n",
      "Epoch 400 | Train: 3.4027e-02 | Val: 4.2119e-02 | LR: 3.91e-06\n",
      "Epoch 410 | Train: 3.4155e-02 | Val: 4.2174e-02 | LR: 1.95e-06\n",
      "Epoch 420 | Train: 3.4118e-02 | Val: 4.2191e-02 | LR: 1.95e-06\n",
      "Epoch 430 | Train: 3.4002e-02 | Val: 4.2167e-02 | LR: 9.77e-07\n",
      "Early stopping at epoch 434\n",
      "\n",
      "Evaluating correction for GCN + KL Divergence...\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Model + Loss                       MSE          MAE     L2 Error       KL Div    Stoch Err\n",
      "-------------------------------------------------------------------------------------\n",
      "GCN + MSE                   1.2694e-03   2.2923e-02   7.9589e-02   4.6029e-02   3.4869e-08\n",
      "GCN + Stochastic Loss       1.2426e-03   2.2570e-02   7.8459e-02   4.4650e-02   3.4273e-08\n",
      "GCN + L2 Loss               1.5200e-03   2.4829e-02   8.5344e-02   5.0267e-02   3.2634e-08\n",
      "GAT + Stochastic Loss       1.5486e-03   2.6751e-02   9.3927e-02   5.7256e-02   3.6657e-08\n",
      "GIN + Stochastic Loss       1.5474e-03   2.6697e-02   9.4236e-02   5.7389e-02   3.4720e-08\n",
      "GCN + KL Divergence         1.2607e-03   2.2909e-02   7.9267e-02   4.2097e-02   3.2783e-08\n",
      "\n",
      "======================================================================\n",
      "CORRECTION PERFORMANCE (Hellinger Fidelity)\n",
      "======================================================================\n",
      "\n",
      "Model + Loss                  Uncorrected     True Column     Pred Column\n",
      "------------------------------------------------------------------------\n",
      "GCN + MSE                          0.7644          0.6042          0.5942\n",
      "GCN + Stochastic Loss              0.7642          0.5999          0.5959\n",
      "GCN + L2 Loss                      0.7643          0.6084          0.5974\n",
      "GAT + Stochastic Loss              0.7644          0.6019          0.5878\n",
      "GIN + Stochastic Loss              0.7643          0.6068          0.5891\n",
      "GCN + KL Divergence                0.7644          0.6085          0.5950\n",
      "\n",
      "ğŸ† Best Model (by L2 error): GCN + Stochastic Loss\n",
      "   Mean L2 Error: 7.8459e-02\n",
      "   Mean KL Divergence: 4.4650e-02\n",
      "   Stochasticity Error: 3.4273e-08\n",
      "\n",
      "   Correction Performance:\n",
      "   Uncorrected Fidelity: 0.7642 Â± 0.2025\n",
      "   True Column Fidelity: 0.5999 Â± 0.3371\n",
      "   Pred Column Fidelity: 0.5959 Â± 0.3378\n",
      "\n",
      "   Improvement over uncorrected:\n",
      "   True column: -69.7% towards ideal\n",
      "   Pred column: -71.4% towards ideal\n",
      "   Pred achieves 99.3% of true column performance\n",
      "\n",
      "======================================================================\n",
      "DETAILED METRICS\n",
      "======================================================================\n",
      "\n",
      "L2 Error Statistics:\n",
      "  GCN + MSE                : Î¼=7.9589e-02, Ïƒ=6.1814e-02\n",
      "  GCN + Stochastic Loss    : Î¼=7.8459e-02, Ïƒ=6.1520e-02\n",
      "  GCN + L2 Loss            : Î¼=8.5344e-02, Ïƒ=6.9833e-02\n",
      "  GAT + Stochastic Loss    : Î¼=9.3927e-02, Ïƒ=5.9723e-02\n",
      "  GIN + Stochastic Loss    : Î¼=9.4236e-02, Ïƒ=5.9150e-02\n",
      "  GCN + KL Divergence      : Î¼=7.9267e-02, Ïƒ=6.1663e-02\n",
      "\n",
      "KL Divergence Statistics:\n",
      "  GCN + MSE                : Î¼=4.6029e-02, Ïƒ=5.6037e-02\n",
      "  GCN + Stochastic Loss    : Î¼=4.4650e-02, Ïƒ=5.6321e-02\n",
      "  GCN + L2 Loss            : Î¼=5.0267e-02, Ïƒ=6.5080e-02\n",
      "  GAT + Stochastic Loss    : Î¼=5.7256e-02, Ïƒ=5.9476e-02\n",
      "  GIN + Stochastic Loss    : Î¼=5.7389e-02, Ïƒ=5.8936e-02\n",
      "  GCN + KL Divergence      : Î¼=4.2097e-02, Ïƒ=5.6354e-02\n",
      "\n",
      "Total Variation Distance:\n",
      "  GCN + MSE                : 9.1691e-02\n",
      "  GCN + Stochastic Loss    : 9.0279e-02\n",
      "  GCN + L2 Loss            : 9.9317e-02\n",
      "  GAT + Stochastic Loss    : 1.0700e-01\n",
      "  GIN + Stochastic Loss    : 1.0679e-01\n",
      "  GCN + KL Divergence      : 9.1637e-02\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "The GNN approach demonstrates effective noise correction using FWHT:\n",
      "\n",
      "1. Column Prediction Performance:\n",
      "   - Best model: GCN + Stochastic Loss\n",
      "   - Mean L2 error: 7.8459e-02\n",
      "   - Stochasticity maintained: 3.4273e-08\n",
      "\n",
      "2. Correction Performance:\n",
      "   - Uncorrected fidelity: 0.7642\n",
      "   - True column correction: 0.5999 (-69.7% improvement)\n",
      "   - Predicted column correction: 0.5959 (-71.4% improvement)\n",
      "   - Model achieves 99.3% of oracle performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from qiskit_ibm_runtime.fake_provider import FakeAthensV2\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main comparison script\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings\n",
    "    n_qubits = 3\n",
    "    depth = 5\n",
    "    n_samples = 2000\n",
    "    shots = 2048\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (FAKE ATHENS)\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Build dataset\n",
    "    print(\"\\nBuilding graph dataset...\")\n",
    "    \n",
    "    # Instantiate the fake backend object\n",
    "    athens_backend = FakeAthensV2()\n",
    "\n",
    "    dec_computer = DECMatrixComputer(backend=athens_backend, shots=shots, twirl=True)\n",
    "    \n",
    "    graphs, columns, circuits, ideal_dists = build_graph_dataset(\n",
    "         dec_computer, n_qubits, depth, n_samples, shots=shots, transpile_circuit=True\n",
    "    )\n",
    "    print(f\"Dataset built: {len(graphs)} circuits\")\n",
    "    print(f\"Node features dim: {graphs[0].x.shape[1]}\")\n",
    "    print(f\"Target dimension: {2**n_qubits}\")\n",
    "    \n",
    "    n_train = int(0.8 * n_samples)\n",
    "    val_graphs = graphs[n_train:]\n",
    "    val_columns = columns[n_train:]\n",
    "    val_circuits = circuits[n_train:]\n",
    "    val_ideal_dists = ideal_dists[n_train:]\n",
    "\n",
    "    results = {}\n",
    "    correction_results = {}\n",
    "    \n",
    "    # Test different combinations\n",
    "    experiments = [\n",
    "        (\"GNN\", \"mse\", \"GCN + MSE\"),\n",
    "        (\"GNN\", \"stochastic\", \"GCN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"l2\", \"GCN + L2 Loss\"),\n",
    "        (\"GAT\", \"stochastic\", \"GAT + Stochastic Loss\"),\n",
    "        (\"GIN\", \"stochastic\", \"GIN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"kl\", \"GCN + KL Divergence\")\n",
    "    ]\n",
    "    \n",
    "    for model_type, loss_type, name in experiments:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Training {name}...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        model, train_losses, val_losses = train_column_gnn(\n",
    "            graphs, n_qubits, n_train=n_train, epochs=2000,\n",
    "            batch_size=32, lr=1e-3, model_type=model_type, loss_type=loss_type\n",
    "        )\n",
    "        \n",
    "        metrics, preds, true = evaluate_column_model(model, val_graphs, n_qubits)\n",
    "        results[name] = metrics\n",
    "        \n",
    "        # Evaluate correction performance\n",
    "        print(f\"\\nEvaluating correction for {name}...\")\n",
    "        corr_res, corr_summary = evaluate_correction(\n",
    "            model, val_graphs, val_columns, val_circuits, val_ideal_dists,\n",
    "            n_qubits, dec_computer, shots\n",
    "        )\n",
    "        correction_results[name] = corr_summary\n",
    "\n",
    "    # Print comprehensive results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n{:<25} {:>12} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "        \"Model + Loss\", \"MSE\", \"MAE\", \"L2 Error\", \"KL Div\", \"Stoch Err\"\n",
    "    ))\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        print(\"{:<25} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e}\".format(\n",
    "            name,\n",
    "            metrics['mse'],\n",
    "            metrics['mae'],\n",
    "            metrics['mean_l2'],\n",
    "            metrics['mean_kl'],\n",
    "            metrics['stochastic_error']\n",
    "        ))\n",
    "    \n",
    "    # Print correction results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CORRECTION PERFORMANCE (Hellinger Fidelity)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n{:<25} {:>15} {:>15} {:>15}\".format(\n",
    "        \"Model + Loss\", \"Uncorrected\", \"True Column\", \"Pred Column\"\n",
    "    ))\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    for name, corr in correction_results.items():\n",
    "        print(\"{:<25} {:>15.4f} {:>15.4f} {:>15.4f}\".format(\n",
    "            name,\n",
    "            corr['mean_fidelity_uncorrected'],\n",
    "            corr['mean_fidelity_true_correction'],\n",
    "            corr['mean_fidelity_pred_correction']\n",
    "        ))\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mean_l2'])\n",
    "    print(f\"\\nğŸ† Best Model (by L2 error): {best_model_name}\")\n",
    "    print(f\"   Mean L2 Error: {results[best_model_name]['mean_l2']:.4e}\")\n",
    "    print(f\"   Mean KL Divergence: {results[best_model_name]['mean_kl']:.4e}\")\n",
    "    print(f\"   Stochasticity Error: {results[best_model_name]['stochastic_error']:.4e}\")\n",
    "    \n",
    "    # Best correction performance\n",
    "    best_corr = correction_results[best_model_name]\n",
    "    print(f\"\\n   Correction Performance:\")\n",
    "    print(f\"   Uncorrected Fidelity: {best_corr['mean_fidelity_uncorrected']:.4f} Â± {best_corr['std_fidelity_uncorrected']:.4f}\")\n",
    "    print(f\"   True Column Fidelity: {best_corr['mean_fidelity_true_correction']:.4f} Â± {best_corr['std_fidelity_true_correction']:.4f}\")\n",
    "    print(f\"   Pred Column Fidelity: {best_corr['mean_fidelity_pred_correction']:.4f} Â± {best_corr['std_fidelity_pred_correction']:.4f}\")\n",
    "    \n",
    "    # Improvement metrics\n",
    "    uncorr_fid = best_corr['mean_fidelity_uncorrected']\n",
    "    true_fid = best_corr['mean_fidelity_true_correction']\n",
    "    pred_fid = best_corr['mean_fidelity_pred_correction']\n",
    "    \n",
    "    true_improvement = (true_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_improvement = (pred_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_vs_true = pred_fid / true_fid * 100\n",
    "    \n",
    "    print(f\"\\n   Improvement over uncorrected:\")\n",
    "    print(f\"   True column: {true_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred column: {pred_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred achieves {pred_vs_true:.1f}% of true column performance\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nL2 Error Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_l2']:.4e}, Ïƒ={metrics['std_l2']:.4e}\")\n",
    "    \n",
    "    print(\"\\nKL Divergence Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_kl']:.4e}, Ïƒ={metrics['std_kl']:.4e}\")\n",
    "    \n",
    "    print(\"\\nTotal Variation Distance:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: {metrics['mean_tv']:.4e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\"\"\n",
    "The GNN approach demonstrates effective noise correction using FWHT:\n",
    "\n",
    "1. Column Prediction Performance:\n",
    "   - Best model: {best_model_name}\n",
    "   - Mean L2 error: {results[best_model_name]['mean_l2']:.4e}\n",
    "   - Stochasticity maintained: {results[best_model_name]['stochastic_error']:.4e}\n",
    "\n",
    "2. Correction Performance:\n",
    "   - Uncorrected fidelity: {uncorr_fid:.4f}\n",
    "   - True column correction: {true_fid:.4f} ({true_improvement:.1f}% improvement)\n",
    "   - Predicted column correction: {pred_fid:.4f} ({pred_improvement:.1f}% improvement)\n",
    "   - Model achieves {pred_vs_true:.1f}% of oracle performance\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

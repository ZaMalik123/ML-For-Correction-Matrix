{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a31c58-ac32-4e6d-8102-d21630e193fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Enhanced GNN Pipeline for First Column Learning\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (GCNConv, GATConv, GINConv, global_mean_pool, \n",
    "                                global_max_pool, global_add_pool, MessagePassing)\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d, LayerNorm\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_aer.noise import depolarizing_error, NoiseModel\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from qiskit_ibm_runtime import Sampler, Session, Options\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "  \n",
    "def hellinger_fidelity(p, q):\n",
    "    \"\"\"Compute Hellinger fidelity between two probability distributions.\n",
    "\n",
    "    The Hellinger fidelity is defined as:\n",
    "        F_H(p, q) = sum_i sqrt(p_i * q_i)\n",
    "\n",
    "    It ranges from 0 (completely different) to 1 (identical).\n",
    "\n",
    "    Args:\n",
    "        p: First probability distribution\n",
    "        q: Second probability distribution\n",
    "\n",
    "    Returns:\n",
    "        Hellinger fidelity (scalar in [0, 1])\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "\n",
    "    # Ensure non-negative\n",
    "    p = np.maximum(p, 0)\n",
    "    q = np.maximum(q, 0)\n",
    "\n",
    "    # Normalize\n",
    "    if np.sum(p) > 0:\n",
    "        p = p / np.sum(p)\n",
    "    if np.sum(q) > 0:\n",
    "        q = q / np.sum(q)\n",
    "\n",
    "    return np.sum(np.sqrt(p * q))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Original helper functions\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def pauli_noise_model(p1=0.01, p2=0.1):\n",
    "    nm = NoiseModel()\n",
    "    one_q_gates = [\"h\",\"s\",\"t\",\"x\",\"y\",\"z\",\"rz\",\"sx\"]\n",
    "    two_q_gates = [\"cx\",\"cz\"]\n",
    "    one_q_err = depolarizing_error(p1, 1)\n",
    "    two_q_err = depolarizing_error(p2, 2)\n",
    "    nm.add_all_qubit_quantum_error(one_q_err, one_q_gates)\n",
    "    nm.add_all_qubit_quantum_error(two_q_err, two_q_gates)\n",
    "    return nm\n",
    "\n",
    "def get_ideal_distribution(circuit, shots=8192):\n",
    "    \"\"\"Get ideal (noiseless) distribution from circuit.\"\"\"\n",
    "    sim = AerSimulator()\n",
    "    full_circuit = circuit.copy()\n",
    "    full_circuit.measure_all()\n",
    "    \n",
    "    result = sim.run(full_circuit, shots=shots).result()\n",
    "    counts = result.get_counts()\n",
    "    \n",
    "    n = circuit.num_qubits\n",
    "    dist = np.zeros(2**n, dtype=np.float32)\n",
    "    for bitstring, c in counts.items():\n",
    "        idx = int(bitstring, 2)\n",
    "        dist[idx] = c / shots\n",
    "    \n",
    "    return dist\n",
    "\n",
    "def random_STHCZ_circuit(n_qubits: int, depth_layers: int, p_cz=0.5):\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    oneq_gates = [\"h\",\"s\",\"t\"]\n",
    "    for _ in range(depth_layers):\n",
    "        for q in range(n_qubits):\n",
    "            g = np.random.choice(oneq_gates)\n",
    "            getattr(qc, g)(q)\n",
    "        if np.random.rand() < p_cz and n_qubits >= 2:\n",
    "            a, b = np.random.choice(n_qubits, size=2, replace=False)\n",
    "            qc.cz(a, b)\n",
    "        qc.barrier()\n",
    "    return qc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Enhanced Circuit to Graph Conversion\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def circuit_to_graph(circuit: QuantumCircuit, max_depth: int = 20) -> Data:\n",
    "    \"\"\"Convert circuit to graph with rich features.\"\"\"\n",
    "    n_qubits = circuit.num_qubits\n",
    "    \n",
    "    # Extended gate types\n",
    "    gate_types = [\"h\", \"s\", \"t\", \"x\", \"y\", \"z\", \"rz\", \"sx\", \"cx\", \"cz\", \"barrier\", \"other\"]\n",
    "    gate_to_idx = {g: i for i, g in enumerate(gate_types)}\n",
    "    \n",
    "    node_features = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    \n",
    "    last_gate_on_qubit = [-1] * n_qubits\n",
    "    gate_depth = [0] * n_qubits\n",
    "    \n",
    "    gate_idx = 0\n",
    "    total_depth = 0\n",
    "    \n",
    "    for inst, qargs, cargs in circuit.data:\n",
    "        if inst.name == \"barrier\":\n",
    "            continue\n",
    "            \n",
    "        gate_name = inst.name if inst.name in gate_to_idx else \"other\"\n",
    "        \n",
    "        # Calculate local depth\n",
    "        involved_qubits = [circuit.qubits.index(q) for q in qargs]\n",
    "        local_depth = max(gate_depth[q] for q in involved_qubits) + 1\n",
    "        total_depth = max(total_depth, local_depth)\n",
    "        \n",
    "        for q in involved_qubits:\n",
    "            gate_depth[q] = local_depth\n",
    "        \n",
    "        # Node features\n",
    "        gate_one_hot = [0] * len(gate_types)\n",
    "        gate_one_hot[gate_to_idx[gate_name]] = 1\n",
    "        \n",
    "        qubit_vec = [0] * n_qubits\n",
    "        for q in qargs:\n",
    "            qi = circuit.qubits.index(q)\n",
    "            qubit_vec[qi] = 1\n",
    "        \n",
    "        # Additional features\n",
    "        arity = inst.num_qubits\n",
    "        is_entangling = 1 if inst.num_qubits > 1 else 0\n",
    "        is_clifford = 1 if gate_name in [\"h\", \"s\", \"cx\", \"cz\"] else 0\n",
    "        is_phase_gate = 1 if gate_name in [\"s\", \"t\", \"rz\"] else 0\n",
    "        normalized_depth = local_depth / max(max_depth, 1)\n",
    "        \n",
    "        # For first column: distance from input matters\n",
    "        distance_from_input = local_depth / max(total_depth, 1) if total_depth > 0 else 0\n",
    "        \n",
    "        node_feat = (gate_one_hot + qubit_vec + \n",
    "                    [arity, is_entangling, is_clifford, is_phase_gate, \n",
    "                     normalized_depth, distance_from_input])\n",
    "        node_features.append(node_feat)\n",
    "        \n",
    "        # Edges\n",
    "        for q in qargs:\n",
    "            qi = circuit.qubits.index(q)\n",
    "            if last_gate_on_qubit[qi] != -1:\n",
    "                edge_index.append([last_gate_on_qubit[qi], gate_idx])\n",
    "                edge_attr.append([1, 0, normalized_depth])  # [sequential, entangling, depth]\n",
    "                \n",
    "                edge_index.append([gate_idx, last_gate_on_qubit[qi]])\n",
    "                edge_attr.append([1, 0, normalized_depth])\n",
    "            last_gate_on_qubit[qi] = gate_idx\n",
    "        \n",
    "        # Entangling edges\n",
    "        if inst.num_qubits == 2:\n",
    "            for i, q in enumerate(involved_qubits):\n",
    "                for j, other_q in enumerate(involved_qubits):\n",
    "                    if i != j and last_gate_on_qubit[other_q] != -1:\n",
    "                        edge_index.append([gate_idx, last_gate_on_qubit[other_q]])\n",
    "                        edge_attr.append([0, 1, normalized_depth])\n",
    "        \n",
    "        gate_idx += 1\n",
    "    \n",
    "    # Global features\n",
    "    global_features = [\n",
    "        n_qubits / 10,\n",
    "        total_depth / 20,\n",
    "        gate_idx / 100,\n",
    "        len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n",
    "    ]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if len(node_features) == 0:\n",
    "        feature_dim = len(gate_types) + n_qubits + 6\n",
    "        x = torch.zeros((1, feature_dim), dtype=torch.float32)\n",
    "        edge_index_tensor = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr_tensor = torch.zeros((0, 3), dtype=torch.float32)\n",
    "    else:\n",
    "        x = torch.tensor(node_features, dtype=torch.float32)\n",
    "        if len(edge_index) > 0:\n",
    "            edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "            edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "        else:\n",
    "            edge_index_tensor = torch.zeros((2, 0), dtype=torch.long)\n",
    "            edge_attr_tensor = torch.zeros((0, 3), dtype=torch.float32)\n",
    "    \n",
    "    global_feat_tensor = torch.tensor(global_features, dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index_tensor, edge_attr=edge_attr_tensor,\n",
    "                global_features=global_feat_tensor)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Dataset builder\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from dec_matrix_computer_V2 import DECMatrixComputer\n",
    "\n",
    "\n",
    "def build_graph_dataset(dec_computer, n_qubits=3, depth_layers=6, n_samples=1000,\n",
    "                       shots=8192, transpile_circuit=False, basis_gates=None, twirl=True, backend=None):\n",
    "    \"\"\"Build dataset with graph representations and first column targets.\n",
    "    If transpile_circuit=True, then we use graph representations of the transpiled circuit.\n",
    "    If twirl=True, then transpile_circuit must also be true.    \n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    columns = []\n",
    "    circuits = []\n",
    "    ideal_dists = []\n",
    "    noisy_dists = []\n",
    "\n",
    "    if backend is None:\n",
    "        backend = AerSimulator()\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate circuit\n",
    "        circ_logical = random_STHCZ_circuit(n_qubits, depth_layers)\n",
    "\n",
    "        if transpile_circuit:\n",
    "            if basis_gates is None:\n",
    "                basis_gates = ['cx', 'id', 'rz', 'sx', 'x']  # IBM basis\n",
    "                \n",
    "            circ = transpile(\n",
    "                circ_logical, \n",
    "                basis_gates=basis_gates,\n",
    "                optimization_level=0  # No optimization to preserve structure\n",
    "            )\n",
    "        else:\n",
    "            circ = circ_logical\n",
    "            \n",
    "        # Convert to graph\n",
    "        graph = circuit_to_graph(circ, max_depth=depth_layers)\n",
    "\n",
    "        # Get noisy distribution\n",
    "        if twirl: # Sample from twirled circuit\n",
    "            noisy_dist = dec_computer.get_full_twirled_distribution(\n",
    "                    circ_logical,  # Pass the 1-qubit logical circuit\n",
    "                    num_gate_randomizations=20, # 20 gate twirls\n",
    "                    num_meas_randomizations=20  # 20 meas twirls (20*20*shots_per)\n",
    "                                                # This part is a bit tricky.\n",
    "                                                # A simpler way is to redefine total_shots\n",
    "                    )\n",
    "        else:\n",
    "            noisy_dist = dec_computer.sample_circuit(circ)  # Sample circuit from dec_computer\n",
    "\n",
    "        # Get first column (for NEC)\n",
    "        first_col = dec_computer.compute_first_column_fast(circ)\n",
    "        \n",
    "        # Get ideal distribution\n",
    "        ideal_dist = get_ideal_distribution(circ, shots)\n",
    "        \n",
    "        # Store\n",
    "        graph.y = torch.tensor(first_col, dtype=torch.float32).unsqueeze(0)\n",
    "        graphs.append(graph)\n",
    "        columns.append(first_col)\n",
    "        circuits.append(circ)\n",
    "        ideal_dists.append(ideal_dist)\n",
    "        noisy_dists.append(noisy_dist)\n",
    "    \n",
    "    return graphs, columns, circuits, ideal_dists, noisy_dists\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Training function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_column_gnn(graphs, n_qubits, n_train=800, epochs=200, batch_size=32,\n",
    "                    lr=1e-3, model_type=\"GNN\", loss_type=\"mse\",\n",
    "                    weight_decay=1e-4, scheduler_patience=10):\n",
    "    \"\"\"Train GNN for column prediction.\"\"\"\n",
    "    \n",
    "    from graph_networks import ColumnGNN, ColumnGAT, ColumnGIN\n",
    "    from losses import ColumnL2Loss, StochasticColumnLoss, KLDivergenceLoss\n",
    "    \n",
    "    # Split data\n",
    "    train_graphs = graphs[:n_train]\n",
    "    val_graphs = graphs[n_train:]\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model dimensions\n",
    "    node_feat_dim = train_graphs[0].x.shape[1]\n",
    "    output_dim = 2 ** n_qubits\n",
    "    hidden_dim = 128\n",
    "    global_feat_dim = train_graphs[0].global_features.shape[0]\n",
    "    \n",
    "    # Initialize model\n",
    "    if model_type == \"GNN\":\n",
    "        model = ColumnGNN(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    elif model_type == \"GAT\":\n",
    "        model = ColumnGAT(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    elif model_type == \"GIN\":\n",
    "        model = ColumnGIN(node_feat_dim, hidden_dim, output_dim, global_feat_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Loss function\n",
    "    if loss_type == \"l2\":\n",
    "        loss_fn = ColumnL2Loss()\n",
    "    elif loss_type == \"stochastic\":\n",
    "        loss_fn = StochasticColumnLoss()\n",
    "    elif loss_type == \"kl\":\n",
    "        loss_fn = KLDivergenceLoss()\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=scheduler_patience, factor=0.5\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_size_actual = batch.num_graphs\n",
    "            global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "            \n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            target = batch.y.view(-1, output_dim)\n",
    "            \n",
    "            loss = loss_fn(out, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "        train_loss /= len(train_graphs)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_size_actual = batch.num_graphs\n",
    "                global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "                \n",
    "                out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "                target = batch.y.view(-1, output_dim)\n",
    "                \n",
    "                loss = loss_fn(out, target)\n",
    "                val_loss += loss.item() * batch.num_graphs\n",
    "        val_loss /= len(val_graphs)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | Train: {train_loss:.4e} | Val: {val_loss:.4e} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 30:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Evaluation function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_column_model(model, val_graphs, n_qubits):\n",
    "    \"\"\"Evaluate column prediction model.\"\"\"\n",
    "    model.eval()\n",
    "    val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    output_dim = 2 ** n_qubits\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_size_actual = batch.num_graphs\n",
    "            global_feats = batch.global_features.view(batch_size_actual, -1)\n",
    "            \n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            target = batch.y.view(-1, output_dim)\n",
    "            \n",
    "            all_preds.append(out.cpu().numpy())\n",
    "            all_true.append(target.cpu().numpy())\n",
    "    \n",
    "    Y_pred = np.concatenate(all_preds, axis=0)\n",
    "    Y_true = np.concatenate(all_true, axis=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # MSE and MAE\n",
    "    metrics['mse'] = np.mean((Y_pred - Y_true) ** 2)\n",
    "    metrics['mae'] = np.mean(np.abs(Y_pred - Y_true))\n",
    "    \n",
    "    # L2 norm error\n",
    "    l2_errors = np.sqrt(np.sum((Y_pred - Y_true) ** 2, axis=1))\n",
    "    metrics['mean_l2'] = np.mean(l2_errors)\n",
    "    metrics['std_l2'] = np.std(l2_errors)\n",
    "    \n",
    "    # Stochasticity error\n",
    "    sum_pred = np.sum(Y_pred, axis=1)\n",
    "    metrics['stochastic_error'] = np.mean(np.abs(sum_pred - 1.0))\n",
    "    \n",
    "    # KL divergence (treating as distributions)\n",
    "    kl_divs = []\n",
    "    for i in range(len(Y_pred)):\n",
    "        pred = Y_pred[i] + 1e-10\n",
    "        true = Y_true[i] + 1e-10\n",
    "        pred_norm = pred / pred.sum()\n",
    "        true_norm = true / true.sum()\n",
    "        kl = np.sum(true_norm * np.log(true_norm / pred_norm))\n",
    "        kl_divs.append(kl)\n",
    "    metrics['mean_kl'] = np.mean(kl_divs)\n",
    "    metrics['std_kl'] = np.std(kl_divs)\n",
    "    \n",
    "    # Total variation distance\n",
    "    tv_distances = 0.5 * np.sum(np.abs(Y_pred - Y_true), axis=1)\n",
    "    metrics['mean_tv'] = np.mean(tv_distances)\n",
    "    \n",
    "    return metrics, Y_pred, Y_true\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Correction Evaluation Function\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_correction(model, val_graphs, val_columns, val_circuits, val_ideal_dists, val_noisy_dists,\n",
    "                       n_qubits, dec_computer,shots=8192):\n",
    "    \"\"\"Evaluate correction performance using both true and predicted columns.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with Hellinger fidelities before and after correction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    results = {\n",
    "        'fidelity_uncorrected': [],\n",
    "        'fidelity_true_correction': [],\n",
    "        'fidelity_pred_correction': []\n",
    "    }\n",
    "    \n",
    "    # Get model predictions\n",
    "    val_loader = DataLoader(val_graphs, batch_size=1, shuffle=False)\n",
    "    pred_columns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            global_feats = batch.global_features.view(1, -1)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch, global_feats)\n",
    "            pred_columns.append(out.cpu().numpy()[0])\n",
    "    \n",
    "    # Evaluate each circuit\n",
    "    for i, (circ, true_col, pred_col, ideal_dist, noisy_dist) in enumerate(zip(\n",
    "            val_circuits, val_columns, pred_columns, val_ideal_dists, val_noisy_dists)):\n",
    "        \n",
    "        # Compute uncorrected fidelity\n",
    "        fid_uncorrected = hellinger_fidelity(noisy_dist, ideal_dist)\n",
    "        results['fidelity_uncorrected'].append(fid_uncorrected)\n",
    "        \n",
    "        # Correct using dec_computer and the correct column\n",
    "        corrected_true = dec_computer.correct_distribution_properly(noisy_dist, true_col)\n",
    "        fid_true = hellinger_fidelity(corrected_true, ideal_dist)\n",
    "        results['fidelity_true_correction'].append(fid_true)\n",
    "\n",
    "        # Correct using dec_computer and the predicted column\n",
    "        corrected_pred = dec_computer.correct_distribution_properly(noisy_dist, pred_col)\n",
    "        fid_pred = hellinger_fidelity(corrected_pred, ideal_dist)\n",
    "        results['fidelity_pred_correction'].append(fid_pred)\n",
    "\n",
    "    # Compute statistics\n",
    "    summary = {\n",
    "        'mean_fidelity_uncorrected': np.mean(results['fidelity_uncorrected']),\n",
    "        'std_fidelity_uncorrected': np.std(results['fidelity_uncorrected']),\n",
    "        'mean_fidelity_true_correction': np.mean(results['fidelity_true_correction']),\n",
    "        'std_fidelity_true_correction': np.std(results['fidelity_true_correction']),\n",
    "        'mean_fidelity_pred_correction': np.mean(results['fidelity_pred_correction']),\n",
    "        'std_fidelity_pred_correction': np.std(results['fidelity_pred_correction']),\n",
    "    }\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1aa3a1-65c0-449c-93a6-c656bbaaaeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (FAKE ATHENS)\n",
      "====================================================================================================\n",
      "\n",
      "Building graph dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_6584\\3849484040.py:117: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  for inst, qargs, cargs in circuit.data:\n",
      "C:\\Users\\zacha\\AppData\\Local\\Temp\\ipykernel_6584\\3849484040.py:181: DeprecationWarning: Treating CircuitInstruction as an iterable is deprecated legacy behavior since Qiskit 1.2, and will be removed in Qiskit 3.0. Instead, use the `operation`, `qubits` and `clbits` named attributes.\n",
      "  len([inst for inst, _, _ in circuit.data if inst.num_qubits > 1]) / max(gate_idx, 1)\n"
     ]
    }
   ],
   "source": [
    "from qiskit_ibm_runtime.fake_provider import FakeAthensV2\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main comparison script\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings\n",
    "    n_qubits = 3\n",
    "    depth = 3\n",
    "    n_samples = 2000\n",
    "    shots = 2048\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"GNN PIPELINE FOR FIRST COLUMN PREDICTION ON TRANSPILED CIRCUITS (FAKE ATHENS)\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Build dataset\n",
    "    print(\"\\nBuilding graph dataset...\")\n",
    "    \n",
    "    # Instantiate the fake backend object\n",
    "    athens_backend = FakeAthensV2()\n",
    "\n",
    "    dec_computer = DECMatrixComputer(backend=athens_backend, shots=shots, twirl=True)\n",
    "    \n",
    "    graphs, columns, circuits, ideal_dists, noisy_dists = build_graph_dataset(\n",
    "         dec_computer, n_qubits, depth, n_samples, shots=shots, transpile_circuit=True, twirl=True, backend=athens_backend\n",
    "    )\n",
    "    print(f\"Dataset built: {len(graphs)} circuits\")\n",
    "    print(f\"Node features dim: {graphs[0].x.shape[1]}\")\n",
    "    print(f\"Target dimension: {2**n_qubits}\")\n",
    "    \n",
    "    n_train = int(0.8 * n_samples)\n",
    "    val_graphs = graphs[n_train:]\n",
    "    val_columns = columns[n_train:]\n",
    "    val_circuits = circuits[n_train:]\n",
    "    val_ideal_dists = ideal_dists[n_train:]\n",
    "    val_noisy_dists = noisy_dists[n_train:]\n",
    "\n",
    "    results = {}\n",
    "    correction_results = {}\n",
    "    \n",
    "    # Test different combinations\n",
    "    experiments = [\n",
    "        (\"GNN\", \"mse\", \"GCN + MSE\"),\n",
    "        (\"GNN\", \"stochastic\", \"GCN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"l2\", \"GCN + L2 Loss\"),\n",
    "        (\"GAT\", \"stochastic\", \"GAT + Stochastic Loss\"),\n",
    "        (\"GIN\", \"stochastic\", \"GIN + Stochastic Loss\"),\n",
    "        (\"GNN\", \"kl\", \"GCN + KL Divergence\")\n",
    "    ]\n",
    "    \n",
    "    for model_type, loss_type, name in experiments:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Training {name}...\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        model, train_losses, val_losses = train_column_gnn(\n",
    "            graphs, n_qubits, n_train=n_train, epochs=2000,\n",
    "            batch_size=32, lr=1e-3, model_type=model_type, loss_type=loss_type\n",
    "        )\n",
    "        \n",
    "        metrics, preds, true = evaluate_column_model(model, val_graphs, n_qubits)\n",
    "        results[name] = metrics\n",
    "        \n",
    "        # Evaluate correction performance\n",
    "        print(f\"\\nEvaluating correction for {name}...\")\n",
    "        corr_res, corr_summary = evaluate_correction(\n",
    "            model, val_graphs, val_columns, val_circuits, \n",
    "            val_ideal_dists, val_noisy_ists, n_qubits, \n",
    "            dec_computer, shots\n",
    "        )\n",
    "        correction_results[name] = corr_summary\n",
    "\n",
    "    # Print comprehensive results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(\"\\n{:<25} {:>12} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "        \"Model + Loss\", \"MSE\", \"MAE\", \"L2 Error\", \"KL Div\", \"Stoch Err\"\n",
    "    ))\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for name, metrics in results.items():\n",
    "        print(\"{:<25} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e} {:>12.4e}\".format(\n",
    "            name,\n",
    "            metrics['mse'],\n",
    "            metrics['mae'],\n",
    "            metrics['mean_l2'],\n",
    "            metrics['mean_kl'],\n",
    "            metrics['stochastic_error']\n",
    "        ))\n",
    "    \n",
    "    # Print correction results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CORRECTION PERFORMANCE (Hellinger Fidelity)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n{:<25} {:>15} {:>15} {:>15}\".format(\n",
    "        \"Model + Loss\", \"Uncorrected\", \"True Column\", \"Pred Column\"\n",
    "    ))\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    for name, corr in correction_results.items():\n",
    "        print(\"{:<25} {:>15.4f} {:>15.4f} {:>15.4f}\".format(\n",
    "            name,\n",
    "            corr['mean_fidelity_uncorrected'],\n",
    "            corr['mean_fidelity_true_correction'],\n",
    "            corr['mean_fidelity_pred_correction']\n",
    "        ))\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mean_l2'])\n",
    "    print(f\"\\nğŸ† Best Model (by L2 error): {best_model_name}\")\n",
    "    print(f\"   Mean L2 Error: {results[best_model_name]['mean_l2']:.4e}\")\n",
    "    print(f\"   Mean KL Divergence: {results[best_model_name]['mean_kl']:.4e}\")\n",
    "    print(f\"   Stochasticity Error: {results[best_model_name]['stochastic_error']:.4e}\")\n",
    "    \n",
    "    # Best correction performance\n",
    "    best_corr = correction_results[best_model_name]\n",
    "    print(f\"\\n   Correction Performance:\")\n",
    "    print(f\"   Uncorrected Fidelity: {best_corr['mean_fidelity_uncorrected']:.4f} Â± {best_corr['std_fidelity_uncorrected']:.4f}\")\n",
    "    print(f\"   True Column Fidelity: {best_corr['mean_fidelity_true_correction']:.4f} Â± {best_corr['std_fidelity_true_correction']:.4f}\")\n",
    "    print(f\"   Pred Column Fidelity: {best_corr['mean_fidelity_pred_correction']:.4f} Â± {best_corr['std_fidelity_pred_correction']:.4f}\")\n",
    "    \n",
    "    # Improvement metrics\n",
    "    uncorr_fid = best_corr['mean_fidelity_uncorrected']\n",
    "    true_fid = best_corr['mean_fidelity_true_correction']\n",
    "    pred_fid = best_corr['mean_fidelity_pred_correction']\n",
    "    \n",
    "    true_improvement = (true_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_improvement = (pred_fid - uncorr_fid) / (1 - uncorr_fid) * 100\n",
    "    pred_vs_true = pred_fid / true_fid * 100\n",
    "    \n",
    "    print(f\"\\n   Improvement over uncorrected:\")\n",
    "    print(f\"   True column: {true_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred column: {pred_improvement:.1f}% towards ideal\")\n",
    "    print(f\"   Pred achieves {pred_vs_true:.1f}% of true column performance\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nL2 Error Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_l2']:.4e}, Ïƒ={metrics['std_l2']:.4e}\")\n",
    "    \n",
    "    print(\"\\nKL Divergence Statistics:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: Î¼={metrics['mean_kl']:.4e}, Ïƒ={metrics['std_kl']:.4e}\")\n",
    "    \n",
    "    print(\"\\nTotal Variation Distance:\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"  {name:<25}: {metrics['mean_tv']:.4e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\"\"\n",
    "The GNN approach demonstrates effective noise correction using FWHT:\n",
    "\n",
    "1. Column Prediction Performance:\n",
    "   - Best model: {best_model_name}\n",
    "   - Mean L2 error: {results[best_model_name]['mean_l2']:.4e}\n",
    "   - Stochasticity maintained: {results[best_model_name]['stochastic_error']:.4e}\n",
    "\n",
    "2. Correction Performance:\n",
    "   - Uncorrected fidelity: {uncorr_fid:.4f}\n",
    "   - True column correction: {true_fid:.4f} ({true_improvement:.1f}% improvement)\n",
    "   - Predicted column correction: {pred_fid:.4f} ({pred_improvement:.1f}% improvement)\n",
    "   - Model achieves {pred_vs_true:.1f}% of oracle performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2e17d-a453-4132-8f48-f883c4326bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
